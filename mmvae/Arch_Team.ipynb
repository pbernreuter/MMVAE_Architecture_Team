{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core VAE Architecture Model:\n",
    "\n",
    "    - Encoder:\n",
    "        - Input vector of size 60664\n",
    "        - Output Mean and Variance vectors of size 64 for latent space\n",
    "\n",
    "    - Decoder:\n",
    "        - Input latent space vector of size 64\n",
    "        - Output Reconstrcuted input of size 60664\n",
    "    \n",
    "    - Using Xavier weight initialization\n",
    "\n",
    "    - Using ReLU activation functions throughout\n",
    "\n",
    "    - Addition of batch normalization layers after each activation\n",
    "\n",
    "    - Usage:\n",
    "        - Set model in trainer: model = Arch_Model.VAE()\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mmvae.models.utils as utils\n",
    "import mmvae.models as M\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        #Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(60664, 1024),\n",
    "            nn.BatchNorm1d(1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #Latent Space\n",
    "        self.fc_mu = nn.Linear(64, 64)\n",
    "        self.fc_var = nn.Linear(64, 64)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 60664),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.encoder)\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.decoder)\n",
    "        utils._submodules_init_weights_xavier_uniform_(self.fc_mu)\n",
    "        utils._xavier_uniform_(self.fc_var, -1.0)\n",
    "\n",
    "    #Call Encoder\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    #Call Decoder\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    #Update parameters\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mArch_Model\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mArch_Model\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_singlechunk_dataloaders\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtb\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVAETrainer\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m#Allow for possibility of sending specfifc hyperparameters into trainer\u001b[39;00m\n",
      "File \u001b[0;32m/active/debruinz_project/kyle_scott/MMVAE_Architecture_Team/mmvae/data/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkedCellCensusDataLoader, MultiModalLoader, configure_singlechunk_dataloaders\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigure_singlechunk_dataloaders\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunkedCellCensusDataLoader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiModalLoader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m/active/debruinz_project/kyle_scott/MMVAE_Architecture_Team/mmvae/data/loaders.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader2, MultiProcessingReadingService, ReadingServiceInterface\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generator, Any\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/active/debruinz_project/kyle_scott/MMVAE_Architecture_Team/mmvae/data/pipes/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCellCensus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CellCensusPipeLine\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCellCensusPipeLine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutils\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m/active/debruinz_project/kyle_scott/MMVAE_Architecture_Team/mmvae/data/pipes/CellCensus.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileLister, IterDataPipe\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sparse_matrix\u001b[39m(file_tuples):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trainer for data:\n",
    "\n",
    "    - Sets Hyperparameters\n",
    "    - Loads data\n",
    "    - trains data\n",
    "    \n",
    "    - Parameters:\n",
    "        - device, model, batch size, learning rate, epochs, starting kl value, \n",
    "            ending kl value, epoch to start annealing kl, number of annealing steps\n",
    "\n",
    "    - Constructor:\n",
    "        - Load model to device\n",
    "        - Set optimizer\n",
    "        - Initialize hyperparameters\n",
    "        - Setup tensorboard writer\n",
    "        - Load data \n",
    "\n",
    "    - Loss function:\n",
    "        - Calulcates MSE loss and KL divergence using mean reduction\n",
    "        - Returns these two values as tuple\n",
    "\n",
    "    - Training loop:\n",
    "        - Iterates over train_loader\n",
    "        - Calculates KL annealing rate\n",
    "        - Calls loss function \n",
    "        - Multiplies annealing rate by kl divergence\n",
    "        - Adds annealed kl loss and MSE loss for total loss\n",
    "        - Preforms norm clipping\n",
    "        - Writes to tensboard\n",
    "\n",
    "    - Annealing KL loss:\n",
    "        - Parameters:\n",
    "            - start_kl: initial weight of KL loss\n",
    "            - end_kl: end weight that KL loss builds up to\n",
    "            - annealing_start: epoch to start adding KL loss \n",
    "            - annealing_steps: number of epochs for KL loss to grow over\n",
    "        \n",
    "        - Training:\n",
    "            - Check for starting epoch \n",
    "            - If current epoch = annealing_start, anneal kl\n",
    "            - If current epoch != annealing_start, kl = 0\n",
    "    \n",
    "    - Usage:\n",
    "        - Create instance of trainer: trainer = VAETrainer(device)\n",
    "        - Call training loop function: trainer.train()\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import mmvae.trainers.utils as utils\n",
    "import torch.nn as nn\n",
    "import mmvae.models.Arch_Model as Arch_Model\n",
    "from mmvae.data import configure_singlechunk_dataloaders\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "class VAETrainer:\n",
    "    #Allow for possibility of sending specfifc hyperparameters into trainer\n",
    "    def __init__(self, device, model=Arch_Model.VAE(), batch_size=128, learning_rate=0.0001, num_epochs=10, start_kl=0.0, end_kl=1.0, annealing_start=0, annealing_steps=10):\n",
    "        #Configure\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        #Hyperparameters\n",
    "        self.lr = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.start_kl = start_kl #Initial Weight of KL loss\n",
    "        self.end_kl = end_kl    #End weight of KL loss\n",
    "        self.annealing_start = annealing_start  #Specify what epoch to start annealing kl \n",
    "        self.annealing_steps = annealing_steps  #Specify number of steps to anneal kl over\n",
    "        #Tensorboard writer \n",
    "        self.writer = tb.SummaryWriter()\n",
    "        #Load Data\n",
    "        self.train_loader = configure_singlechunk_dataloaders(\n",
    "            data_file_path='/active/debruinz_project/CellCensus_3M_Full/3m_human_full.npz',\n",
    "            metadata_file_path=None,\n",
    "            train_ratio=1,\n",
    "            batch_size=self.batch_size,\n",
    "            device=None\n",
    "        )\n",
    "\n",
    "    #Loss function returning MSE and KL divergence as a tuple using mean reduction\n",
    "    def loss_function(self, recon_x, x: torch.Tensor, mu, logvar):\n",
    "        reconstruction_loss = F.mse_loss(recon_x, x.to_dense(), reduction='mean') \n",
    "        kl_divergence = (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())).mean()\n",
    "        return reconstruction_loss, kl_divergence\n",
    "\n",
    "    #Training loop\n",
    "    def train(self):\n",
    "        print(\"Start Training ....\")\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i, (x, _) in enumerate(self.train_loader):\n",
    "                x = x.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_batch, mu, logvar = self.model(x)\n",
    "\n",
    "                #Check starting epoch for kl\n",
    "                annealing = 0\n",
    "                if epoch >= self.annealing_start:\n",
    "                    annealing_ratio = (epoch - self.annealing_start) / self.annealing_steps\n",
    "                    annealing = self.start_kl + annealing_ratio * (self.end_kl - self.start_kl)\n",
    "                \n",
    "                #Combine MSE and KL for total loss (kl * 0 if not at starting epoch)\n",
    "                recon_loss, kl_loss = self.loss_function(recon_batch, x, mu, logvar)\n",
    "                annealing_kl = kl_loss * annealing\n",
    "                loss = recon_loss + annealing_kl\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) #Gradient Norm Clipping\n",
    "                self.optimizer.step()\n",
    "\n",
    "                #Tensorboard total loss over iterations\n",
    "                self.writer.add_scalar('Loss/Iteration', loss.item(), epoch * len(self.train_loader) + i) \n",
    "\n",
    "            #Write to tensorboard  \n",
    "            self.writer.add_scalar('Annealing Schedule', annealing, epoch) \n",
    "            self.writer.add_scalar('Loss/KL', kl_loss.item(), epoch)\n",
    "            self.writer.add_scalar('Loss/MSE', recon_loss.item(), epoch)\n",
    "            self.writer.add_scalar('Loss/Total', loss.item(), epoch)\n",
    "            \n",
    "        print(\"done training\")\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mMain file to call trainer:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    - Sets device to cuda if available, if not use CPU\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    - Creater trainer and call train function\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mArch_Trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VAETrainer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main file to call trainer:\n",
    "\n",
    "    - Sets device to cuda if available, if not use CPU\n",
    "    - Creater trainer and call train function\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from mmvae.trainers.Arch_Trainer import VAETrainer\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "    trainer = VAETrainer(device)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
